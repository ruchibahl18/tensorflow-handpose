<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Display Webcam Stream</title>

<style>
#videoElement {
	width: 500px;
	height: 375px;
	background-color: #666;
	display: none;
}
</style>
</head>
 
<body>
	<video autoplay="true" id="videoElement"></video>
	<canvas id='canvas' width = '600px' height='400px'></canvas>
	<!-- Require the peer dependencies of handpose. -->
	<script src="https://unpkg.com/@tensorflow/tfjs-core@2.1.0/dist/tf-core.js"></script>
	<script src="https://unpkg.com/@tensorflow/tfjs-converter@2.1.0/dist/tf-converter.js"></script>

	<!-- You must explicitly require a TF.js backend if you're not using the tfs union bundle. -->
	<script src="https://unpkg.com/@tensorflow/tfjs-backend-webgl@2.1.0/dist/tf-backend-webgl.js"></script>
	<!-- Alternatively you can use the WASM backend: <script src="https://unpkg.com/@tensorflow/tfjs-backend-wasm@2.1.0/dist/tf-backend-wasm.js"></script> -->

	<script src="https://unpkg.com/@tensorflow-models/handpose@0.0.6/dist/handpose.js"></script>
<script>
	  
	let video = document.querySelector("#videoElement");
	let model;
	let canvas = document.querySelector("#canvas");
	let ctx = canvas.getContext('2d');
	let fingerLookupIndices = {
      thumb: [0, 1, 2, 3, 4],
      indexFinger: [0, 5, 6, 7, 8],
      middleFinger: [0, 9, 10, 11, 12],
      ringFinger: [0, 13, 14, 15, 16],
      pinky: [0, 17, 18, 19, 20]
    }; 
	
	function drawPoint(y, x, r) {
	  ctx.beginPath();
	  ctx.arc(x, y, r, 0, 2 * Math.PI);
	  ctx.fill();
	}

	function drawKeypoints(keypoints) {
	  const keypointsArray = keypoints;

	  for (let i = 0; i < keypointsArray.length; i++) {
		const y = keypointsArray[i][0];
		const x = keypointsArray[i][1];
		drawPoint(x - 2, y - 2, 3);
	  }

	  const fingers = Object.keys(fingerLookupIndices);
	  for (let i = 0; i < fingers.length; i++) {
		const finger = fingers[i];
		const points = fingerLookupIndices[finger].map(idx => keypoints[idx]);
		drawPath(points, false);
	  }
	}

	function drawPath(points, closePath) {
	  const region = new Path2D();
	  region.moveTo(points[0][0], points[0][1]);
	  for (let i = 1; i < points.length; i++) {
		const point = points[i];
		region.lineTo(point[0], point[1]);
	  }

	  if (closePath) {
		region.closePath();
	  }
	  ctx.stroke(region);
	}

	const setupCamera = () => {
		if (navigator.mediaDevices.getUserMedia) {
		  navigator.mediaDevices.getUserMedia({ video: true })
			.then(function (stream) {
			  video.srcObject = stream;
			})
			.catch(function (err0r) {
			  console.log("Something went wrong!");
			});
		}
	}
	const detectFaces = async () => {
		const prediction = await model.estimateHands(video);
		console.log(prediction);
		ctx.drawImage(video,0,0,600,400);
		ctx.beginPath();
		ctx.lineWidth = "4";
		ctx.strokeStyle = "blue";
		prediction.forEach((pred) => {
			const result = pred.landmarks;
			drawKeypoints(result, pred.annotations);
		});
	};
	setupCamera();
	video.addEventListener("loadeddata", async () => {
		model = await handpose.load();
		setInterval(detectFaces,100);
	});
</script>
</body>
</html>